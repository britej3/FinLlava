# Complete Self-Improving Multimodal Trading Agent Blueprint (Revised)

## Core Architecture Overview

**Base Philosophy**: Three-tier autonomous system with two-stage model development
- **AI Engineer (MLE-STAR)**: Code generation and refinement agents
- **Trading Brain (HRM/ZRIA)**: Multimodal reasoning engine with custom architecture
- **Learning Engine (DRL)**: Continuous improvement through market interaction

**Strategic Approach**: Two-Stage Model Development
- **Stage 1**: Prototype with efficient 2.7B model (Weeks 1-8)
- **Stage 2**: Scale to production with 7B+ model (Weeks 9-12)

---

## Phase 1: Environment Setup & Foundation

### Hardware Specifications

#### Stage 1 Development (Weeks 1-8)
- **Primary GPU**: NVIDIA RTX 4090 (24GB VRAM) - $1,600
- **Alternative**: RTX 4080 (16GB VRAM) - $1,200
- **Budget Cloud**: Vast.ai RTX 4090 at $0.50/hour
- **CPU**: 16+ cores (Intel i7-12700K or AMD Ryzen 9 5950X)
- **RAM**: 64GB DDR4 (upgrade to 128GB if processing tick data)
- **Storage**: 2TB NVMe SSD (Samsung 980 Pro or WD Black SN850X)

#### Stage 2 Production (Weeks 9-12)
- **Cloud GPU**: AWS EC2 g5.4xlarge (A10G 24GB) - $1.20/hour
- **Premium**: Google Cloud A100 (40GB) - $3.00/hour
- **Local Upgrade**: RTX 4090 sufficient for 7B models with quantization

### Software Stack Installation

```bash
# Core Environment Setup
conda create -n trading_agent python=3.10 && conda activate trading_agent

# Deep Learning & Transformers
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.35.0 accelerate bitsandbytes
pip install huggingface_hub datasets peft
pip install flash-attn --no-build-isolation  # For memory optimization

# Reinforcement Learning
pip install ray[rllib]==2.8.0 gymnasium==0.29.1
pip install stable-baselines3==2.2.1 tensorboard

# Trading & Market Data
pip install ccxt==4.1.0 backtrader vectorbt ta-lib
pip install zipline-reloaded freqtrade yfinance
pip install python-binance alpha_vantage websocket-client

# Data Processing & ML
pip install pandas==2.1.3 numpy==1.24.3 polars
pip install scikit-learn==1.3.2 xgboost==2.0.1 lightgbm
pip install ta tsfresh featuretools optuna hyperopt

# Quantization & Memory Optimization
pip install auto-gptq optimum
pip install onnx onnxruntime-gpu

# Visualization & Monitoring
pip install mplfinance plotly streamlit dash
pip install prometheus_client loguru sentry-sdk wandb

# Fine-tuning & Annotation
pip install label-studio dvc[s3] unstructured
pip install nlpaug albumentations datasets
```

### Project Directory Structure
```
/self_improving_trading_agent/
├── agents/
│   ├── __init__.py
│   ├── base_agent.py
│   ├── coder_agent.py
│   ├── feature_engineer_agent.py
│   ├── backtest_agent.py
│   └── ablation_agent.py
├── src/
│   ├── __init__.py
│   ├── multimodal_llm.py
│   ├── hrm_agent.py
│   ├── zria_blocks.py
│   ├── drl_environment.py
│   ├── feature_engineering.py
│   └── safety_wrapper.py
├── data/
│   ├── raw/
│   │   ├── market_data/
│   │   ├── alternative_data/
│   │   └── educational_content/
│   ├── processed/
│   │   ├── instruction_pairs/
│   │   ├── multimodal_examples/
│   │   └── features/
│   └── models/
├── config/
│   ├── stage1_config.yaml
│   ├── stage2_config.yaml
│   └── deployment_config.yaml
├── notebooks/
├── backtests/
├── logs/
├── main_bootstrap.py
├── main_stage1_training.py
├── main_stage2_upgrade.py
└── main_mle_loop.py
```

---

## Phase 2: Two-Stage Model Strategy

### Stage 1: Development Model
```python
stage_1_config = {
    "base_model": "salesforce/blip2-opt-2.7b",
    "parameters": "2.7B",
    "vram_requirement": "6-8GB",
    "inference_speed": "<100ms",
    "training_time": "2-3 hours/epoch",
    "purpose": "System development and validation",
    "quantization": "4-bit via bitsandbytes"
}
```

### Stage 2: Production Model
```python
stage_2_config = {
    "base_model": "microsoft/DialoGPT-large",  # Alternative to FinLLaVA
    "vision_model": "microsoft/git-base-coco",
    "combined_approach": "Custom fusion architecture",
    "parameters": "7B equivalent",
    "vram_requirement": "16-20GB",
    "inference_speed": "200-400ms", 
    "training_time": "8-12 hours/epoch",
    "purpose": "Production trading performance"
}
```

### Model Selection Rationale

#### Why Not FinLLaVA-7B Directly
1. **Licensing Concerns**: Some LLaVA derivatives have restrictive licenses
2. **Training Data Bias**: Pre-trained on general finance, not crypto-specific
3. **Customization Flexibility**: Building from components gives more control

#### Our Hybrid Approach
```python
class MultimodalTradingLLM:
    def __init__(self, stage="stage1"):
        if stage == "stage1":
            self.text_model = AutoModelForCausalLM.from_pretrained(
                "salesforce/blip2-opt-2.7b",
                quantization_config=BitsAndBytesConfig(load_in_4bit=True)
            )
            self.vision_processor = BlipProcessor.from_pretrained("salesforce/blip2-opt-2.7b")
            
        elif stage == "stage2":
            # Scale to larger, more capable models
            self.text_model = AutoModelForCausalLM.from_pretrained(
                "microsoft/DialoGPT-large",
                torch_dtype=torch.float16
            )
            self.vision_model = AutoModelForVisionTextDualEncoding.from_pretrained(
                "microsoft/git-base-coco"
            )
            self.fusion_layer = self._build_custom_fusion()
    
    def _build_custom_fusion(self):
        """Custom architecture for combining vision and text representations"""
        return nn.Sequential(
            nn.Linear(1024 + 768, 512),  # vision_dim + text_dim -> hidden
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)  # Final trading decision embedding
        )
```

---

## Phase 3: Educational Foundation Dataset

### Fundamental Trading Knowledge Sources (All FREE)
```python
educational_sources = {
    "academic_papers": {
        "arxiv_quantfin": "https://arxiv.org/list/q-fin/recent",
        "ssrn_microstructure": "https://www.ssrn.com/en/index.cfm/janda/",
        "researchgate_crypto": "Free research papers on crypto markets"
    },
    "professional_books": {
        "murphy_technical_analysis": "Archive.org PDF",
        "chan_algorithmic_trading": "Free chapters + summaries",
        "lewis_flash_boys": "Market microstructure insights"
    },
    "crypto_specific": {
        "binance_academy": "Complete educational series",
        "coinmarketcap_learn": "Free crypto trading courses",
        "messari_research": "Free tier reports",
        "github_repositories": "Open source trading strategies"
    },
    "community_wisdom": {
        "reddit_algotrading": "Use PRAW API for historical posts",
        "quantconnect_forums": "Web scraping with respect for ToS",
        "stackoverflow_quant": "Tagged questions on quantitative finance"
    }
}
```

### Instruction Tuning Dataset Structure
```python
# Target: 15,000+ high-quality examples
dataset_composition = {
    "fundamental_concepts": {
        "examples": 3000,
        "topics": [
            "Order book dynamics and Level II interpretation",
            "Perpetual futures and funding rate mechanics", 
            "Cross-exchange arbitrage identification",
            "Liquidation cascade patterns and stop hunting",
            "Options flow and gamma/delta hedging impacts"
        ]
    },
    "chart_analysis": {
        "examples": 4000,
        "topics": [
            "Support/resistance level identification",
            "Pattern recognition (triangles, flags, wedges)",
            "Volume profile analysis",
            "Multi-timeframe confluence",
            "Market structure breaks"
        ]
    },
    "risk_management": {
        "examples": 2500,
        "topics": [
            "Position sizing algorithms",
            "Dynamic stop-loss placement",
            "Portfolio heat management",
            "Correlation-based exposure limits",
            "Drawdown recovery strategies"
        ]
    },
    "market_microstructure": {
        "examples": 2000,
        "topics": [
            "Bid-ask spread analysis",
            "Order flow imbalance detection",
            "Market maker behavior patterns",
            "Latency arbitrage opportunities",
            "Cross-market inefficiencies"
        ]
    },
    "synthetic_scenarios": {
        "examples": 3500,
        "generation_method": "Monte Carlo market simulations",
        "focus": "Edge cases and rare market conditions"
    }
}
```

### Data Generation Pipeline
```python
class EducationalDatasetGenerator:
    def __init__(self):
        self.content_scrapers = {
            "arxiv": ArxivScraper(),
            "reddit": RedditScraper(api_key="your_praw_key"),
            "binance_academy": BinanceAcademyScraper()
        }
        self.llm_generator = BaseAgent("mistralai/Mistral-7B-Instruct-v0.2")
        
    def generate_instruction_pairs(self):
        """Generate instruction-response pairs from raw content"""
        raw_content = self.collect_raw_educational_content()
        
        for content in raw_content:
            # Use LLM to convert content into Q&A format
            prompt = f"""
            Convert this educational content into 3 instruction-response pairs for training a trading AI:
            
            Content: {content}
            
            Format each pair as:
            Instruction: [Clear question or task]
            Response: [Detailed, actionable answer with reasoning]
            """
            
            pairs = self.llm_generator.run(prompt)
            yield self.parse_and_validate_pairs(pairs)
```

---

## Phase 4: Data Pipeline & Market Sources

### Primary Market Data APIs
```python
market_data_config = {
    "primary_exchange": {
        "platform": "Binance",
        "api": "python-binance",
        "cost": "FREE", 
        "rate_limits": {
            "rest": "1200 requests/minute",
            "websocket": "5 connections/IP"
        },
        "data_types": [
            "OHLCV (1m to 1d intervals)",
            "Order Book (Level 2, 5000 depth)",
            "Aggregate Trades (real-time)",
            "Funding Rates (8-hour updates)",
            "24hr Ticker Statistics"
        ]
    },
    "alternatives": {
        "coinbase_pro": "Free, 10 requests/second",
        "kraken": "Free, 1 request/second", 
        "deribit": "Options data, free tier available"
    },
    "paper_trading": {
        "platform": "Alpaca Markets",
        "cost": "FREE",
        "features": "Real-time paper trading with market data"
    }
}
```

### Alternative Data Integration
```python
alternative_data_sources = {
    "sentiment_analysis": {
        "twitter_api_v2": {
            "cost": "10,000 tweets/month FREE",
            "usage": "Crypto sentiment from #BTC, #crypto hashtags",
            "rate_limit": "300 requests/15min"
        },
        "reddit_api": {
            "cost": "FREE via PRAW",
            "subreddits": ["r/Bitcoin", "r/CryptoCurrency", "r/algotrading"],
            "data": "Post sentiment, comment analysis"
        }
    },
    "news_feeds": {
        "alpha_vantage": {
            "cost": "500 calls/day FREE",
            "endpoint": "NEWS_SENTIMENT",
            "coverage": "Crypto and general market news"
        },
        "newsapi": {
            "cost": "1000 requests/day FREE", 
            "sources": "Financial news outlets"
        }
    },
    "onchain_metrics": {
        "glassnode": {
            "cost": "$39/month (basic tier)",
            "metrics": "Bitcoin network health, exchange flows",
            "roi_justification": "High-alpha signal for BTC trading"
        },
        "free_alternatives": {
            "blockchain_info": "Basic Bitcoin metrics (FREE)",
            "coinmetrics": "Limited free tier"
        }
    },
    "economic_indicators": {
        "fred_api": {
            "cost": "FREE",
            "data": "Federal Reserve economic data",
            "relevance": "DXY, interest rates, inflation"
        }
    }
}
```

### Data Processing & Storage Architecture
```python
class DataPipeline:
    def __init__(self):
        self.storage = {
            "raw_data": "PostgreSQL (time-series optimized)",
            "processed_features": "Redis (fast access)", 
            "model_data": "Parquet files (columnar storage)",
            "backup": "S3-compatible storage"
        }
        
    def setup_database(self):
        """Configure time-series database for market data"""
        return {
            "postgresql_config": {
                "extensions": ["TimescaleDB"],
                "tables": {
                    "ohlcv_1m": "Minute candles, partitioned by date",
                    "orderbook_snapshots": "L2 data, compressed",
                    "trades": "Individual trade records",
                    "features": "Engineered indicators", 
                    "signals": "Trading decisions and outcomes"
                }
            },
            "redis_config": {
                "use_case": "Real-time feature serving",
                "data_types": "Latest indicators, live signals",
                "memory": "16GB allocated"
            }
        }
```

---

## Phase 5: Agent Architecture Implementation

### Improved Base Agent (Fixed)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import re

class BaseAgent:
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.2", stage="stage1"):
        self.model_name = model_name
        self.stage = stage
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # Configure for development vs production stage
        if stage == "stage1":
            # Memory-optimized loading for development
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            # Full precision for production
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16
        )
    
    def run(self, prompt, max_tokens=1500, temperature=0.2):
        """Generate response using proper chat formatting"""
        try:
            # Format as conversation
            messages = [{"role": "user", "content": prompt}]
            
            # Use chat template if available
            if hasattr(self.tokenizer, 'apply_chat_template'):
                formatted_prompt = self.tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            else:
                # Fallback for models without chat template
                formatted_prompt = f"<s>[INST] {prompt} [/INST]"
            
            # Generate response
            outputs = self.pipe(
                formatted_prompt,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.95,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # Clean the output
            full_response = outputs[0]['generated_text']
            response = full_response[len(formatted_prompt):].strip()
            
            return response
            
        except Exception as e:
            return f"Error in agent response: {str(e)}"
```

### Specialized Agents
```python
class CoderAgent(BaseAgent):
    def __init__(self, stage="stage1"):
        model = "codellama/CodeLlama-13b-Instruct-hf" if stage == "stage2" else "codellama/CodeLlama-7b-Instruct-hf"
        super().__init__(model_name=model, stage=stage)
        
        self.system_prompt = """You are an expert PyTorch developer specializing in quantitative finance.
        
        Guidelines:
        - Write clean, well-documented code
        - Include proper error handling
        - Use type hints where appropriate
        - Optimize for both performance and readability
        - Follow PyTorch best practices for neural networks
        """
    
    def generate_pytorch_module(self, description):
        """Generate a complete PyTorch nn.Module"""
        prompt = f"""{self.system_prompt}
        
        Task: Implement the following as a PyTorch nn.Module class:
        
        Description: {description}
        
        Requirements:
        - Complete implementation with __init__ and forward methods
        - Proper docstrings explaining parameters and functionality
        - Input/output tensor shape comments
        - Any necessary helper methods
        
        Code:"""
        
        response = self.run(prompt, max_tokens=2000, temperature=0.1)
        
        # Extract code block
        code_match = re.search(r'```python\n(.*?)```', response, re.DOTALL)
        return code_match.group(1) if code_match else response

class FeatureEngineerAgent(BaseAgent):
    def __init__(self, stage="stage1"):
        super().__init__(model_name="mistralai/Mistral-7B-Instruct-v0.2", stage=stage)
        
    def create_indicator(self, market_context, innovation_focus):
        """Generate new technical indicators based on market context"""
        prompt = f"""You are a quantitative analyst creating novel technical indicators.
        
        Market Context: {market_context}
        Innovation Focus: {innovation_focus}
        
        Create a new technical indicator function with these requirements:
        - Uses pandas DataFrame with OHLCV data
        - Returns a Series with the indicator values
        - Handles NaN values appropriately
        - Includes a 2-3 sentence explanation of the trading logic
        - Function name should be descriptive
        
        Base data available: df['Open'], df['High'], df['Low'], df['Close'], df['Volume']
        
        Python function:"""
        
        return self.run(prompt, max_tokens=1500, temperature=0.3)

class BacktestAgent(BaseAgent):
    def __init__(self, stage="stage1"):
        super().__init__(model_name="mistralai/Mistral-7B-Instruct-v0.2", stage=stage)
        
    def analyze_strategy(self, backtest_results):
        """Analyze backtest results and suggest improvements"""
        prompt = f"""Analyze this trading strategy backtest results:
        
        Results: {backtest_results}
        
        Provide:
        1. Performance assessment (2-3 sentences)
        2. Key strengths (1-2 points)  
        3. Main weaknesses (1-2 points)
        4. Specific improvement suggestions (2-3 concrete recommendations)
        
        Focus on actionable insights for strategy refinement."""
        
        return self.run(prompt, max_tokens=1000, temperature=0.2)
```

---

## Phase 6: Custom HRM/ZRIA Architecture

### ZRIA Block Implementation
```python
class ProbabilisticFractalActivation(nn.Module):
    """P-FAF: Probabilistic Fractal Activation Function"""
    def __init__(self, input_dim, num_fractals=5):
        super().__init__()
        self.input_dim = input_dim
        self.num_fractals = num_fractals
        
        # Dynamic weight generation network
        self.weight_generator = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, num_fractals),
            nn.Softmax(dim=-1)
        )
        
        # Fractal transformation parameters
        self.fractal_params = nn.Parameter(torch.randn(num_fractals, 3))  # scale, phase, frequency
        
    def forward(self, x):
        """
        Args:
            x: Input tensor of shape (batch, seq_len, input_dim)
        Returns:
            Enhanced tensor with fractal activations applied
        """
        # Stabilize input
        x_stable = torch.sigmoid(x)
        
        # Generate dynamic weights from global context
        global_context = x.mean(dim=1, keepdim=True)  # (batch, 1, input_dim)
        fractal_weights = self.weight_generator(global_context)  # (batch, 1, num_fractals)
        
        # Apply fractal functions
        fractal_responses = []
        for i in range(self.num_fractals):
            scale, phase, freq = self.fractal_params[i]
            
            # Different fractal functions
            if i % 3 == 0:
                fractal = torch.sin(freq * x_stable + phase) * scale
            elif i % 3 == 1:
                fractal = torch.cos(freq * x_stable + phase) * scale
            else:
                fractal = torch.tanh(freq * x_stable + phase) * scale
                
            fractal_responses.append(fractal)
        
        # Weighted combination
        fractal_stack = torch.stack(fractal_responses, dim=-1)  # (batch, seq, dim, num_fractals)
        weighted_fractals = torch.sum(fractal_stack * fractal_weights.unsqueeze(-2), dim=-1)
        
        # Residual connection
        return x + weighted_fractals

class FractalAttentionalResonance(nn.Module):
    """FAR: Fractal Attentional Resonance"""
    def __init__(self, d_model, num_heads=8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # Standard attention components
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model) 
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
        # Fractal bias generation
        self.context_processor = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, num_heads),
            nn.Tanh()
        )
        
    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor (batch, seq_len, d_model)
            mask: Optional attention mask
        """
        batch_size, seq_len, _ = x.shape
        
        # Generate Q, K, V
        Q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # Transpose for attention computation
        Q = Q.transpose(1, 2)  # (batch, heads, seq_len, head_dim)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Generate fractal bias from global context
        global_context = x.mean(dim=1)  # (batch, d_model)
        fractal_bias = self.context_processor(global_context)  # (batch, num_heads)
        fractal_bias = fractal_bias.unsqueeze(-1).unsqueeze(-1)  # (batch, heads, 1, 1)
        
        # Add fractal bias to attention scores
        scores = scores + fractal_bias
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        # Softmax and apply to values
        attention_weights = torch.softmax(scores, dim=-1)
        out = torch.matmul(attention_weights, V)
        
        # Transpose back and concatenate heads
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        
        return self.out_linear(out)
```

### Hierarchical Reasoning Module (HRM)
```python
class HierarchicalReasoningModule(nn.Module):
    """HRM: Multi-timescale trading brain"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # Multi-timescale processing
        self.timescale_processors = nn.ModuleDict({
            'micro': self._build_processor(config.micro_dim, 'micro'),      # 1-5 minute
            'short': self._build_processor(config.short_dim, 'short'),      # 15-60 minute  
            'medium': self._build_processor(config.medium_dim, 'medium'),   # 4-24 hour
            'long': self._build_processor(config.long_dim, 'long')          # 1-7 day
        })
        
        # Cross-timescale fusion
        self.fusion_attention = FractalAttentionalResonance(config.fusion_dim)
        self.fusion_projector = nn.Linear(
            config.micro_dim + config.short_dim + config.medium_dim + config.long_dim,
            config.fusion_dim
        )
        
        # Decision head
        self.decision_head = nn.Sequential(
            ProbabilisticFractalActivation(config.fusion_dim),
            nn.Linear(config.fusion_dim, config.fusion_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(config.fusion_dim // 2, config.num_actions)
        )
        
    def _build_processor(self, dim, timescale):
        """Build processor for specific timescale"""
        return nn.Sequential(
            nn.Linear(self.config.input_dim, dim),
            ProbabilisticFractalActivation(dim),
            FractalAttentionalResonance(dim, num_heads=8),
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU()
        )
    
    def forward(self, market_data):
        """
        Args:
            market_data: Dict with different timescale data
                - 'micro': (batch, seq_len, features)
                - 'short': (batch, seq_len, features)  
                - 'medium': (batch, seq_len, features)
                - 'long': (batch, seq_len, features)
        """
        # Process each timescale
        timescale_outputs = {}
        for timescale, processor in self.timescale_processors.items():
            if timescale in market_data:
                timescale_outputs[timescale] = processor(market_data[timescale])
        
        # Concatenate and fuse
        combined = torch.cat(list(timescale_outputs.values()), dim=-1)
        fused = self.fusion_projector(combined)
        
        # Apply cross-timescale attention
        enhanced = self.fusion_attention(fused)
        
        # Generate trading decision
        decision = self.decision_head(enhanced)
        
        return {
            'action_logits': decision,
            'timescale_representations': timescale_outputs,
            'fused_representation': enhanced
        }
```

---

## Phase 7: DRL Training Configuration (Fixed)

### Custom Model Integration with Ray RLlib
```python
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.utils.annotations import override
from ray.rllib.models import ModelCatalog

class HRM_RLLibModel(TorchModelV2, nn.Module):
    """Custom HRM model for Ray RLlib integration"""
    
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)
        
        # Initialize HRM configuration
        hrm_config = SimpleNamespace(
            input_dim=obs_space.shape[0],
            micro_dim=256,
            short_dim=128, 
            medium_dim=64,
            long_dim=32,
            fusion_dim=128,
            num_actions=num_outputs
        )
        
        # Initialize HRM trading brain
        self.hrm_agent = HierarchicalReasoningModule(hrm_config)
        
        # Value function head (required by RLlib)
        self.value_head = nn.Sequential(
            nn.Linear(hrm_config.fusion_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self._features = None
        
    @override(TorchModelV2)
    def forward(self, input_dict, state, seq_lens):
        """Forward pass for policy and value estimation"""
        obs = input_dict["obs"]
        
        # Reshape observation for multi-timescale processing
        market_data = self._parse_observation(obs)
        
        # Forward through HRM
        hrm_output = self.hrm_agent(market_data)
        
        # Store features for value function
        self._features = hrm_output['fused_representation'].mean(dim=1)  # Pool over sequence
        
        # Return action logits
        action_logits = hrm_output['action_logits'].mean(dim=1)  # Pool over sequence
        
        return action_logits, state
    
    @override(TorchModelV2) 
    def value_function(self):
        """Estimate state value for PPO"""
        if self._features is None:
            return torch.zeros(1)
        return self.value_head(self._features).squeeze(-1)
    
    def _parse_observation(self, obs):
        """Parse flat observation into multi-timescale data"""
        # Assuming observation contains concatenated timescale data
        # This would be customized based on your environment's observation space
        
        batch_size = obs.shape[0]
        feature_dim = obs.shape[1] // 4  # Assuming 4 timescales
        
        return {
            'micro': obs[:, :feature_dim].unsqueeze(1),      # Add sequence dim
            'short': obs[:, feature_dim:2*feature_dim].unsqueeze(1),
            'medium': obs[:, 2*feature_dim:3*feature_dim].unsqueeze(1),
            'long': obs[:, 3*feature_dim:].unsqueeze(1)
        }

# Register custom model with RLlib
ModelCatalog.register_custom_model("hrm_trading_model", HRM_RLLibModel)
```

### Complete Training Configuration
```python
import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig

# Training configuration for both stages
def get_training_config(stage="stage1"):
    """Get stage-appropriate training configuration"""
    
    base_config = (
        PPOConfig()
        .environment(env="TradingEnv-v1")
        .framework("torch")
        .training(
            # Model configuration
            model={
                "custom_model": "hrm_trading_model",
                "custom_model_config": {
                    "stage": stage,
                    "use_attention": True,
                    "fractal_activation": True
                }
            },
            
            # PPO hyperparameters
            lr=3e-4 if stage == "stage1" else 1e-4,  # Lower LR for larger model
            train_batch_size=32768 if stage == "stage1" else 16384,
            sgd_minibatch_size=1024 if stage == "stage1" else 512,
            num_sgd_iter=10,
            gamma=0.99,
            lambda_=0.95,
            clip_param=0.2,
            vf_loss_coeff=0.5,
            entropy_coeff=0.01,
            
            # Mixed precision training
            _enable_learner_api=True,
            _enable_rl_module_api=True
        )
        .resources(
            num_gpus=1,
            num_cpus_per_worker=2,
            num_gpus_per_worker=0.1 if stage == "stage1" else 0.2
        )
        .rollouts(
            num_rollout_workers=8 if stage == "stage1" else 4,
            rollout_fragment_length=200,
            batch_mode="complete_episodes"
        )
        .evaluation(
            evaluation_interval=10,
            evaluation_duration=20,
            evaluation_config={"explore": False}
        )
    )
    
    return base_config

# Training execution
def train_trading_agent(stage="stage1"):
    """Train the trading agent"""
    
    # Initialize Ray
    ray.init(ignore_reinit_error=True)
    
    # Get configuration
    config = get_training_config(stage)
    
    # Create trainer
    trainer = config.build()
    
    # Training loop
    max_iterations = 2000 if stage == "stage1" else 1000
    checkpoint_freq = 50
    
    best_reward = float('-inf')
    
    for i in range(max_iterations):
        result = trainer.train()
        
        # Log progress
        episode_reward = result["episode_reward_mean"]
        print(f"Iteration {i}: Reward = {episode_reward:.2f}")
        
        # Save best model
        if episode_reward > best_reward:
            best_reward = episode_reward
            checkpoint_path = trainer.save()
            print(f"New best model saved: {checkpoint_path}")
        
        # Regular checkpointing
        if i % checkpoint_freq == 0:
            trainer.save()
            
        # Early stopping for stage 1
        if stage == "stage1" and best_reward > 2.0:  # Sharpe ratio > 2
            print("Stage 1 target achieved, ready for stage 2")
            break
    
    trainer.stop()
    ray.shutdown()
    
    return checkpoint_path
```

### Intelligent Reward Function
```python
class TradingRewardCalculator:
    """Advanced reward calculation for trading DRL"""
    
    def __init__(self):
        self.lookback_window = 100
        self.risk_free_rate = 0.02  # 2% annual
        
    def calculate_reward(self, returns, positions, market_data, metadata):
        """Calculate comprehensive trading reward"""
        
        # Primary performance metrics
        sharpe_ratio = self._calculate_sharpe(returns)
        max_drawdown = self._calculate_max_drawdown(returns)
        
        # Risk penalties
        drawdown_penalty = self._drawdown_penalty(max_drawdown)
        position_size_penalty = self._position_size_penalty(positions)
        
        # Transaction costs
        transaction_costs = self._calculate_transaction_costs(positions, market_data)
        
        # Market regime adaptation
        volatility_regime = self._detect_market_regime(market_data)
        regime_multiplier = self._get_regime_multiplier(volatility_regime)
        
        # Consistency reward (prefer steady gains over volatile spikes)
        consistency_bonus = self._consistency_reward(returns)
        
        # Base reward calculation
        base_reward = sharpe_ratio * regime_multiplier
        
        # Apply penalties and bonuses
        final_reward = (
            base_reward
            + consistency_bonus
            - drawdown_penalty
            - position_size_penalty
            - transaction_costs
        )
        
        # Logging for analysis
        reward_breakdown = {
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'drawdown_penalty': drawdown_penalty,
            'transaction_costs': transaction_costs,
            'consistency_bonus': consistency_bonus,
            'final_reward': final_reward
        }
        
        return final_reward, reward_breakdown
    
    def _calculate_sharpe(self, returns):
        """Calculate rolling Sharpe ratio"""
        if len(returns) < 30:  # Minimum sample size
            return 0.0
            
        recent_returns = returns[-self.lookback_window:]
        excess_returns = recent_returns - (self.risk_free_rate / 252)  # Daily risk-free rate
        
        if recent_returns.std() == 0:
            return 0.0
            
        return excess_returns.mean() / (recent_returns.std() + 1e-8)
    
    def _calculate_max_drawdown(self, returns):
        """Calculate maximum drawdown"""
        cumulative = (1 + returns).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max
        return abs(drawdown.min())
    
    def _drawdown_penalty(self, max_drawdown):
        """Exponential penalty for large drawdowns"""
        if max_drawdown > 0.20:  # 20% drawdown
            return -50  # Severe penalty
        elif max_drawdown > 0.15:  # 15% drawdown
            return -20
        elif max_drawdown > 0.10:  # 10% drawdown
            return -5
        return 0
    
    def _position_size_penalty(self, positions):
        """Penalty for excessive position sizes"""
        max_position = abs(positions).max()
        if max_position > 0.5:  # 50% of capital
            return -10
        elif max_position > 0.3:  # 30% of capital
            return -2
        return 0
    
    def _calculate_transaction_costs(self, positions, market_data):
        """Estimate transaction costs based on position changes"""
        position_changes = np.diff(positions, prepend=0)
        volume = np.abs(position_changes).sum()
        
        # Assume 0.1% transaction cost
        spread_cost = volume * 0.001
        
        # Market impact (more costly during high volatility)
        volatility = market_data['close'].pct_change().std()
        impact_cost = volume * volatility * 0.1
        
        return spread_cost + impact_cost
    
    def _detect_market_regime(self, market_data):
        """Detect current market regime"""
        returns = market_data['close'].pct_change().dropna()
        
        if len(returns) < 20:
            return 'normal'
        
        recent_vol = returns.tail(20).std()
        long_vol = returns.tail(100).std() if len(returns) >= 100 else recent_vol
        
        if recent_vol > 1.5 * long_vol:
            return 'high_volatility'
        elif recent_vol < 0.7 * long_vol:
            return 'low_volatility'
        else:
            return 'normal'
    
    def _get_regime_multiplier(self, regime):
        """Adjust reward based on market regime"""
        multipliers = {
            'high_volatility': 0.8,  # Harder to trade, lower expectations
            'normal': 1.0,
            'low_volatility': 1.2    # Easier conditions, higher expectations
        }
        return multipliers.get(regime, 1.0)
    
    def _consistency_reward(self, returns):
        """Reward consistent performance over volatile gains"""
        if len(returns) < 30:
            return 0
            
        recent_returns = returns[-30:]
        
        # Positive return rate
        positive_rate = (recent_returns > 0).mean()
        
        # Return stability (lower std relative to mean is better)
        if recent_returns.mean() > 0:
            stability = recent_returns.mean() / (recent_returns.std() + 1e-8)
            consistency_score = positive_rate * min(stability, 2.0)  # Cap at 2.0
            return consistency_score * 0.1  # Small but meaningful bonus
        
        return 0
```

---

## Phase 8: Self-Improvement Engine

### Enhanced Feature Evolution System
```python
class FeatureEvolutionEngine:
    """Genetic algorithm for evolving trading features"""
    
    def __init__(self, population_size=100, mutation_rate=0.15, crossover_rate=0.7):
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        
        # Feature building blocks
        self.indicators = ['sma', 'ema', 'rsi', 'macd', 'bollinger', 'stoch', 'williams_r']
        self.lookback_periods = [5, 10, 14, 20, 21, 50, 100, 200]
        self.operations = ['add', 'subtract', 'multiply', 'divide', 'ratio']
        
        # Current population of feature sets
        self.population = []
        self.fitness_scores = []
        self.generation = 0
        
    def initialize_population(self):
        """Create initial random population"""
        for _ in range(self.population_size):
            feature_set = self._create_random_feature_set()
            self.population.append(feature_set)
        
    def _create_random_feature_set(self):
        """Generate a random feature combination"""
        num_features = np.random.randint(5, 15)  # 5-15 features per set
        
        feature_set = {
            'features': [],
            'metadata': {
                'created': datetime.now(),
                'generation': self.generation
            }
        }
        
        for _ in range(num_features):
            feature_def = {
                'indicator': np.random.choice(self.indicators),
                'period': np.random.choice(self.lookback_periods),
                'operation': np.random.choice(self.operations) if np.random.random() > 0.5 else None,
                'secondary_indicator': np.random.choice(self.indicators) if np.random.random() > 0.7 else None,
                'secondary_period': np.random.choice(self.lookback_periods) if np.random.random() > 0.7 else None
            }
            feature_set['features'].append(feature_def)
        
        return feature_set
    
    def evaluate_population(self, market_data):
        """Evaluate all feature sets using backtesting"""
        self.fitness_scores = []
        
        for i, feature_set in enumerate(self.population):
            try:
                # Generate features
                features_df = self._generate_features(feature_set, market_data)
                
                # Quick backtest
                fitness = self._quick_backtest(features_df, market_data)
                self.fitness_scores.append(fitness)
                
                print(f"Feature set {i+1}/{len(self.population)}: Fitness = {fitness:.3f}")
                
            except Exception as e:
                print(f"Error evaluating feature set {i+1}: {str(e)}")
                self.fitness_scores.append(-10.0)  # Penalty for broken features
    
    def evolve_generation(self):
        """Evolve to next generation"""
        # Selection: Keep top 20% performers
        top_indices = np.argsort(self.fitness_scores)[-int(0.2 * self.population_size):]
        elite_population = [self.population[i] for i in top_indices]
        
        new_population = elite_population.copy()  # Elitism
        
        # Generate offspring to fill population
        while len(new_population) < self.population_size:
            if np.random.random() < self.crossover_rate:
                # Crossover
                parent1, parent2 = np.random.choice(elite_population, 2, replace=False)
                offspring = self._crossover(parent1, parent2)
            else:
                # Mutation only
                parent = np.random.choice(elite_population)
                offspring = self._mutate(parent.copy())
            
            new_population.append(offspring)
        
        self.population = new_population
        self.generation += 1
        
        print(f"Evolved to generation {self.generation}")
        print(f"Best fitness: {max(self.fitness_scores):.3f}")
        print(f"Average fitness: {np.mean(self.fitness_scores):.3f}")
    
    def _crossover(self, parent1, parent2):
        """Combine features from two parents"""
        offspring = {
            'features': [],
            'metadata': {
                'created': datetime.now(),
                'generation': self.generation + 1,
                'parents': [parent1['metadata'], parent2['metadata']]
            }
        }
        
        # Randomly select features from each parent
        all_features = parent1['features'] + parent2['features']
        num_features = min(len(all_features), np.random.randint(8, 20))
        
        selected_features = np.random.choice(
            range(len(all_features)), 
            size=num_features, 
            replace=False
        )
        
        for idx in selected_features:
            offspring['features'].append(all_features[idx])
        
        return offspring
    
    def _mutate(self, individual):
        """Mutate an individual feature set"""
        for feature in individual['features']:
            if np.random.random() < self.mutation_rate:
                # Mutate one aspect of the feature
                mutation_type = np.random.choice(['indicator', 'period', 'operation'])
                
                if mutation_type == 'indicator':
                    feature['indicator'] = np.random.choice(self.indicators)
                elif mutation_type == 'period':
                    feature['period'] = np.random.choice(self.lookback_periods)
                elif mutation_type == 'operation':
                    feature['operation'] = np.random.choice(self.operations + [None])
        
        # Occasionally add or remove features
        if np.random.random() < 0.1:  # 10% chance
            if len(individual['features']) > 5:
                # Remove random feature
                individual['features'].pop(np.random.randint(len(individual['features'])))
            else:
                # Add random feature
                new_feature = {
                    'indicator': np.random.choice(self.indicators),
                    'period': np.random.choice(self.lookback_periods),
                    'operation': np.random.choice(self.operations + [None])
                }
                individual['features'].append(new_feature)
        
        individual['metadata']['generation'] = self.generation + 1
        individual['metadata']['mutated'] = datetime.now()
        
        return individual
    
    def _generate_features(self, feature_set, market_data):
        """Convert feature definition to actual features"""
        import ta  # Technical analysis library
        
        df = market_data.copy()
        feature_columns = []
        
        for feature_def in feature_set['features']:
            try:
                indicator = feature_def['indicator']
                period = feature_def['period']
                
                # Generate base indicator
                if indicator == 'sma':
                    values = df['close'].rolling(period).mean()
                elif indicator == 'ema':
                    values = df['close'].ewm(span=period).mean()
                elif indicator == 'rsi':
                    values = ta.momentum.RSIIndicator(df['close'], window=period).rsi()
                elif indicator == 'macd':
                    macd = ta.trend.MACD(df['close'])
                    values = macd.macd()
                elif indicator == 'bollinger':
                    bb = ta.volatility.BollingerBands(df['close'], window=period)
                    values = (df['close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
                elif indicator == 'stoch':
                    values = ta.momentum.StochasticOscillator(df['high'], df['low'], df['close']).stoch()
                elif indicator == 'williams_r':
                    values = ta.momentum.WilliamsRIndicator(df['high'], df['low'], df['close']).williams_r()
                else:
                    continue
                
                # Apply secondary operations if defined
                if feature_def.get('operation') and feature_def.get('secondary_indicator'):
                    secondary = self._generate_secondary_feature(
                        feature_def['secondary_indicator'],
                        feature_def.get('secondary_period', period),
                        df
                    )
                    
                    operation = feature_def['operation']
                    if operation == 'add':
                        values = values + secondary
                    elif operation == 'subtract':
                        values = values - secondary
                    elif operation == 'multiply':
                        values = values * secondary
                    elif operation == 'divide':
                        values = values / (secondary + 1e-8)  # Avoid division by zero
                    elif operation == 'ratio':
                        values = values / (secondary + 1e-8)
                
                # Add to dataframe
                feature_name = f"{indicator}_{period}"
                if feature_def.get('operation'):
                    feature_name += f"_{feature_def['operation']}_{feature_def.get('secondary_indicator', '')}"
                
                df[feature_name] = values
                feature_columns.append(feature_name)
                
            except Exception as e:
                print(f"Error generating feature {feature_def}: {str(e)}")
                continue
        
        return df[feature_columns].fillna(method='ffill').dropna()
    
    def _quick_backtest(self, features_df, market_data):
        """Fast backtest to evaluate feature set quality"""
        if len(features_df) < 100:  # Minimum data requirement
            return -10.0
        
        # Simple ML model for quick evaluation
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split
        
        # Prepare data
        X = features_df.iloc[:-1]  # Features (except last)
        y = market_data['close'].pct_change().shift(-1).iloc[:-1]  # Next return
        
        # Remove NaN values
        mask = ~(X.isna().any(axis=1) | y.isna())
        X, y = X[mask], y[mask]
        
        if len(X) < 50:
            return -10.0
        
        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, shuffle=False
        )
        
        # Train model
        model = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=5)
        model.fit(X_train, y_train)
        
        # Generate predictions
        predictions = model.predict(X_test)
        
        # Simple trading strategy: long when prediction > 0
        positions = np.where(predictions > 0, 1, -1)
        strategy_returns = positions * y_test.values
        
        # Calculate Sharpe ratio as fitness
        if strategy_returns.std() == 0:
            return -10.0
            
        sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252)  # Annualized
        
        return sharpe
    
    def get_best_feature_set(self):
        """Return the best performing feature set"""
        if not self.fitness_scores:
            return None
            
        best_idx = np.argmax(self.fitness_scores)
        return self.population[best_idx], self.fitness_scores[best_idx]
```

### MLE-STAR Autonomous Loop (Complete)
```python
class MLESTARAutonomousLoop:
    """Complete autonomous improvement system"""
    
    def __init__(self, config):
        self.config = config
        
        # Initialize agents
        self.coder_agent = CoderAgent(stage=config.stage)
        self.feature_engineer = FeatureEngineerAgent(stage=config.stage)
        self.backtest_agent = BacktestAgent(stage=config.stage)
        self.evolution_engine = FeatureEvolutionEngine()
        
        # Performance tracking
        self.performance_history = []
        self.strategy_archive = []
        
        # Safety limits
        self.max_daily_loss = 0.02
        self.min_validation_period = 30  # days
        
    def run_daily_loop(self):
        """Execute daily improvement cycle"""
        try:
            print(f"=== MLE-STAR Daily Loop: {datetime.now()} ===")
            
            # 1. Performance Analysis
            current_performance = self._analyze_current_performance()
            
            # 2. Feature Evolution
            self._evolve_features()
            
            # 3. Strategy Generation
            new_strategies = self._generate_strategy_variations()
            
            # 4. Rapid Testing
            validated_strategies = self._rapid_validation(new_strategies)
            
            # 5. Selection and Deployment
            best_strategy = self._select_best_strategy(validated_strategies)
            
            if self._should_deploy(best_strategy, current_performance):
                self._deploy_new_strategy(best_strategy)
                print(f"✅ Deployed new strategy with Sharpe: {best_strategy['sharpe']:.3f}")
            else:
                print(f"⏸️ No improvement found. Current Sharpe: {current_performance['sharpe']:.3f}")
            
            # 6. Knowledge Update
            self._update_knowledge_base(validated_strategies)
            
            # 7. Archive and Cleanup
            self._archive_results()
            
        except Exception as e:
            print(f"❌ Error in MLE-STAR loop: {str(e)}")
            # Continue with current strategy
    
    def _analyze_current_performance(self):
        """Analyze current strategy performance"""
        # Load recent trading results
        recent_trades = self._load_recent_trades(days=30)
        
        if len(recent_trades) == 0:
            return {'sharpe': 0.0, 'max_dd': 1.0, 'win_rate': 0.0}
        
        returns = recent_trades['pnl'].pct_change().dropna()
        
        performance = {
            'sharpe': self._calculate_sharpe(returns),
            'max_dd': self._calculate_max_drawdown(returns),
            'win_rate': (recent_trades['pnl'] > 0).mean(),
            'total_trades': len(recent_trades),
            'avg_return': returns.mean(),
            'volatility': returns.std()
        }
        
        self.performance_history.append(performance)
        return performance
    
    def _evolve_features(self):
        """Run one iteration of feature evolution"""
        # Load recent market data
        market_data = self._load_market_data(days=90)
        
        # Initialize population if first run
        if not self.evolution_engine.population:
            print("Initializing feature population...")
            self.evolution_engine.initialize_population()
        
        # Evaluate current population
        print("Evaluating feature sets...")
        self.evolution_engine.evaluate_population(market_data)
        
        # Evolve to next generation
        self.evolution_engine.evolve_generation()
        
        # Store best features
        best_features, best_fitness = self.evolution_engine.get_best_feature_set()
        print(f"Best feature set fitness: {best_fitness:.3f}")
        
        return best_features
    
    def _generate_strategy_variations(self, num_variations=10):
        """Generate strategy variations using AI agents"""
        current_strategy = self._load_current_strategy()
        variations = []
        
        for i in range(num_variations):
            try:
                # Generate variation using coder agent
                variation_prompt = f"""
                Current trading strategy performance:
                - Sharpe: {self.performance_history[-1]['sharpe']:.3f}
                - Max Drawdown: {self.performance_history[-1]['max_dd']:.3f}
                - Win Rate: {self.performance_history[-1]['win_rate']:.3f}
                
                Current strategy code:
                {current_strategy}
                
                Generate a variation that addresses the main weakness. 
                Focus on ONE specific improvement:
                1. Better risk management
                2. Improved entry/exit timing
                3. Enhanced position sizing
                4. Better market regime detection
                
                Provide complete Python code for the new strategy.
                """
                
                variation_code = self.coder_agent.generate_pytorch_module(variation_prompt)
                
                variations.append({
                    'code': variation_code,
                    'variation_id': f"var_{i+1}",
                    'generated_at': datetime.now()
                })
                
            except Exception as e:
                print(f"Error generating variation {i+1}: {str(e)}")
                continue
        
        return variations
    
    def _rapid_validation(self, strategies):
        """Fast validation on proxy data"""
        validated = []
        
        # Load proxy dataset (last 30 days)
        proxy_data = self._load_market_data(days=30)
        
        for strategy in strategies:
            try:
                # Quick backtest on proxy data
                result = self._run_proxy_backtest(strategy['code'], proxy_data)
                
                strategy['proxy_sharpe'] = result['sharpe']
                strategy['proxy_max_dd'] = result['max_dd']
                strategy['proxy_trades'] = result['num_trades']
                
                # Only keep strategies that pass basic filters
                if (result['sharpe'] > 0.5 and 
                    result['max_dd'] < 0.20 and 
                    result['num_trades'] > 10):
                    validated.append(strategy)
                
            except Exception as e:
                print(f"Error validating {strategy['variation_id']}: {str(e)}")
                continue
        
        # Sort by proxy performance
        validated.sort(key=lambda x: x['proxy_sharpe'], reverse=True)
        
        return validated[:3]  # Keep top 3 for full validation
    
    def _select_best_strategy(self, validated_strategies):
        """Full validation and selection of best strategy"""
        if not validated_strategies:
            return None
        
        # Full backtest on longer period
        full_validation_data = self._load_market_data(days=180)  # 6 months
        
        best_strategy = None
        best_score = -np.inf
        
        for strategy in validated_strategies:
            try:
                # Full backtest
                result = self._run_full_backtest(strategy['code'], full_validation_data)
                
                # Composite score (Sharpe with penalties)
                score = result['sharpe'] - (result['max_dd'] * 2)  # Penalize drawdown
                
                strategy['full_sharpe'] = result['sharpe']
                strategy['full_max_dd'] = result['max_dd']
                strategy['full_score'] = score
                
                if score > best_score:
                    best_score = score
                    best_strategy = strategy
                
            except Exception as e:
                print(f"Error in full validation: {str(e)}")
                continue
        
        return best_strategy
